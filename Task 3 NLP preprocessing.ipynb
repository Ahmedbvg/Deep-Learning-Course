{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1375839,"sourceType":"datasetVersion","datasetId":164704}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport emoji\n\n# ML libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, f1_score\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T21:59:09.977103Z","iopub.execute_input":"2025-11-28T21:59:09.977631Z","iopub.status.idle":"2025-11-28T21:59:11.097302Z","shell.execute_reply.started":"2025-11-28T21:59:09.977610Z","shell.execute_reply":"2025-11-28T21:59:11.096695Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_pos = pd.read_csv('/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv', sep='\\t', header=None, names=['tweet'])\ntrain_neg = pd.read_csv('/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv', sep='\\t', header=None, names=['tweet'])\ntest_pos = pd.read_csv('/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv', sep='\\t', header=None, names=['tweet'])\ntest_neg = pd.read_csv('/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv', sep='\\t', header=None, names=['tweet'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:00:55.825139Z","iopub.execute_input":"2025-11-28T22:00:55.826043Z","iopub.status.idle":"2025-11-28T22:00:56.085097Z","shell.execute_reply.started":"2025-11-28T22:00:55.826017Z","shell.execute_reply":"2025-11-28T22:00:56.084535Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Add sentiment labels\ntrain_pos['sentiment'] = 'positive'\ntrain_neg['sentiment'] = 'negative'\ntest_pos['sentiment'] = 'positive'\ntest_neg['sentiment'] = 'negative'\n#Combine into single dataset\ndf = pd.concat([train_pos, train_neg, test_pos, test_neg], ignore_index=True)\nprint(df.head())\nprint(df['sentiment'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:01:28.568439Z","iopub.execute_input":"2025-11-28T22:01:28.569056Z","iopub.status.idle":"2025-11-28T22:01:28.593632Z","shell.execute_reply.started":"2025-11-28T22:01:28.569032Z","shell.execute_reply":"2025-11-28T22:01:28.593012Z"}},"outputs":[{"name":"stdout","text":"                                               tweet sentiment\n0  Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...  positive\n1  ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...  positive\n2                                    Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›  positive\n3  #Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...  positive\n4  Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...  positive\nsentiment\npositive    28513\nnegative    28282\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Text Cleaning\ndef clean_text(text):\n    text = str(text)\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+','', text)          # Remove URLs\n    text = re.sub(r'@\\w+','', text)                    # Remove mentions\n    text = re.sub(r'#\\w+','', text)                    # Remove hashtags\n    text = re.sub(r'\\d+','', text)                     # Remove numbers\n    text = re.sub(r'[{}]'.format(string.punctuation), '', text)  # Remove punctuation\n    text = re.sub(r'[Ù‘ÙÙ‹ÙÙŒÙÙÙ’Ù€]', '', text)           # Remove Arabic diacritics\n    text = emoji.replace_emoji(text, replace='')       # Remove emojis\n    text = re.sub(r'\\s+', ' ', text).strip()          # Remove extra spaces\n    return text\n\ndf['clean_text'] = df['tweet'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:02:23.749715Z","iopub.execute_input":"2025-11-28T22:02:23.750012Z","iopub.status.idle":"2025-11-28T22:02:27.266758Z","shell.execute_reply.started":"2025-11-28T22:02:23.749991Z","shell.execute_reply":"2025-11-28T22:02:27.265944Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Feature Extraction \n#TF-IDF\nuse_tfidf = False  # Set True for TF-IDF, False for CountVectorizer + N-grams\n\nif use_tfidf:\n    vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\nelse:\n    vectorizer = CountVectorizer(ngram_range=(1,3), max_features=5000)\n\nX = vectorizer.fit_transform(df['clean_text'])\ny = df['sentiment']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:03:09.196896Z","iopub.execute_input":"2025-11-28T22:03:09.197382Z","iopub.status.idle":"2025-11-28T22:03:12.454875Z","shell.execute_reply.started":"2025-11-28T22:03:09.197358Z","shell.execute_reply":"2025-11-28T22:03:12.454281Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:03:25.144107Z","iopub.execute_input":"2025-11-28T22:03:25.144792Z","iopub.status.idle":"2025-11-28T22:03:25.194325Z","shell.execute_reply.started":"2025-11-28T22:03:25.144769Z","shell.execute_reply":"2025-11-28T22:03:25.193757Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ML Models Training & Evaluation\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Naive Bayes': MultinomialNB(),\n    'Random Forest': RandomForestClassifier(n_estimators=100),\n    'SVM': SVC(kernel='linear')\n}\n\nresults = []\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    results.append({'Model': name, 'Accuracy': acc, 'F1-Score': f1})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:03:52.476471Z","iopub.execute_input":"2025-11-28T22:03:52.476768Z","iopub.status.idle":"2025-11-28T22:12:10.404420Z","shell.execute_reply.started":"2025-11-28T22:03:52.476746Z","shell.execute_reply":"2025-11-28T22:12:10.403570Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#Results Table\nresults_df = pd.DataFrame(results)\nprint(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T22:16:31.931369Z","iopub.execute_input":"2025-11-28T22:16:31.932282Z","iopub.status.idle":"2025-11-28T22:16:31.938953Z","shell.execute_reply.started":"2025-11-28T22:16:31.932254Z","shell.execute_reply":"2025-11-28T22:16:31.938193Z"}},"outputs":[{"name":"stdout","text":"                 Model  Accuracy  F1-Score\n0  Logistic Regression  0.738885  0.737963\n1          Naive Bayes  0.707369  0.707355\n2        Random Forest  0.777533  0.777239\n3                  SVM  0.736244  0.733936\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}